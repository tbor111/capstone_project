{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "import string\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://teresaborcuch@localhost:5433/capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT DISTINCT ON(title) title, date, author, body, link, section FROM ny_times;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5 Million for a Super Bowl Ad. Another Millio...</td>\n",
       "      <td>20170129</td>\n",
       "      <td>Sapna Maheshwari</td>\n",
       "      <td>This month, Anheuser-Busch InBev hosted a doze...</td>\n",
       "      <td>http://www.nytimes.com/2017/01/29/business/5-m...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$60,000 in Tuition, and My Son Wants to Become...</td>\n",
       "      <td>20170112</td>\n",
       "      <td>Philip Galanes</td>\n",
       "      <td>My wife and I are spending a fortune to send o...</td>\n",
       "      <td>http://www.nytimes.com/2017/01/12/fashion/farm...</td>\n",
       "      <td>fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Patient, 7 Tumors and 100 Billion Cells Equa...</td>\n",
       "      <td>20161207</td>\n",
       "      <td>Denise Grady</td>\n",
       "      <td>The remarkable recovery of a woman with advanc...</td>\n",
       "      <td>http://www.nytimes.com/2016/12/07/health/cance...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Unexpected Styling Tricks for Men</td>\n",
       "      <td>20170125</td>\n",
       "      <td>Alex Tudela</td>\n",
       "      <td></td>\n",
       "      <td>http://www.nytimes.com/2017/01/25/t-magazine/f...</td>\n",
       "      <td>t-magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15 of the Best Journals by Our Reporters Aroun...</td>\n",
       "      <td>20161230</td>\n",
       "      <td>Barbara Tierney</td>\n",
       "      <td>Our foreign correspondents wrote about dozens ...</td>\n",
       "      <td>http://www.nytimes.com/2016/12/30/world/15-of-...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      date  \\\n",
       "0  $5 Million for a Super Bowl Ad. Another Millio...  20170129   \n",
       "1  $60,000 in Tuition, and My Son Wants to Become...  20170112   \n",
       "2  1 Patient, 7 Tumors and 100 Billion Cells Equa...  20161207   \n",
       "3               10 Unexpected Styling Tricks for Men  20170125   \n",
       "4  15 of the Best Journals by Our Reporters Aroun...  20161230   \n",
       "\n",
       "             author                                               body  \\\n",
       "0  Sapna Maheshwari  This month, Anheuser-Busch InBev hosted a doze...   \n",
       "1    Philip Galanes  My wife and I are spending a fortune to send o...   \n",
       "2      Denise Grady  The remarkable recovery of a woman with advanc...   \n",
       "3       Alex Tudela                                                      \n",
       "4   Barbara Tierney  Our foreign correspondents wrote about dozens ...   \n",
       "\n",
       "                                                link     section  \n",
       "0  http://www.nytimes.com/2017/01/29/business/5-m...    business  \n",
       "1  http://www.nytimes.com/2017/01/12/fashion/farm...     fashion  \n",
       "2  http://www.nytimes.com/2016/12/07/health/cance...      health  \n",
       "3  http://www.nytimes.com/2017/01/25/t-magazine/f...  t-magazine  \n",
       "4  http://www.nytimes.com/2016/12/30/world/15-of-...       world  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('/Users/teresaborcuch/Desktop/nyt_backup.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in data['title']:\n",
    "    i = i.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = string.punctuation + '’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\xe2\\x80\\x99'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe2\\x80\\x99'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "print 0xe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.encode('ascii', errors = 'replace')\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenize at 0x11a2798c0>, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = TfidfVectorizer(decode_error = 'replace', tokenizer = tokenize, strip_accents = 'unicode', stop_words = 'english')\n",
    "c_vec.fit(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_titles = pd.DataFrame(c_vec.transform(data['title']).todense(), columns = c_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>106</th>\n",
       "      <th>11th</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>16000</th>\n",
       "      <th>18900</th>\n",
       "      <th>...</th>\n",
       "      <th>youv</th>\n",
       "      <th>yuliya</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealou</th>\n",
       "      <th>zenbanx</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhiqiang</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.425791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1        10       100  1000  106  11th        15   16  16000  18900  \\\n",
       "0  0.0000  0.000000  0.000000   0.0  0.0   0.0  0.000000  0.0    0.0    0.0   \n",
       "1  0.0000  0.000000  0.000000   0.0  0.0   0.0  0.000000  0.0    0.0    0.0   \n",
       "2  0.5218  0.000000  0.289898   0.0  0.0   0.0  0.000000  0.0    0.0    0.0   \n",
       "3  0.0000  0.425791  0.000000   0.0  0.0   0.0  0.000000  0.0    0.0    0.0   \n",
       "4  0.0000  0.000000  0.000000   0.0  0.0   0.0  0.519427  0.0    0.0    0.0   \n",
       "\n",
       "  ...   youv  yuliya  zealand  zealou  zenbanx  zero  zhiqiang  zimmerman  \\\n",
       "0 ...    0.0     0.0      0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "1 ...    0.0     0.0      0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "2 ...    0.0     0.0      0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "3 ...    0.0     0.0      0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "4 ...    0.0     0.0      0.0     0.0      0.0   0.0       0.0        0.0   \n",
       "\n",
       "   zombi  zoo  \n",
       "0    0.0  0.0  \n",
       "1    0.0  0.0  \n",
       "2    0.0  0.0  \n",
       "3    0.0  0.0  \n",
       "4    0.0  0.0  \n",
       "\n",
       "[5 rows x 2242 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trump     29.455227\n",
       "ban       10.114447\n",
       "new        9.358870\n",
       "immigr     9.314399\n",
       "order      8.455221\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_titles.sum(axis=0).sort_values(ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make y opinion vs non-opinion\n",
    "y = [1 if i == 'opinion' else 0 for i in data['section']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=3,\n",
       "   estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "   estimator_params=None, scoring='mean_squared_error', step=1, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection\n",
    "rfecv = RFECV(estimator = DecisionTreeClassifier(), cv = 3, scoring = 'mean_squared_error')\n",
    "rfecv.fit(v_titles, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfecv_cols = v_titles.columns[rfecv.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rfecv_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = v_titles[rfecv_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got estimator       america  american  amid  amour  amplifi  analog  ancestor  ancient  \\\n0    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n1    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n2    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n3    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n4    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n5    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n6    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n7    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n8    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n9    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n10   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n11   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n12   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n13   0.361474  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n14   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n15   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n16   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n17   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n18   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n19   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n20   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n21   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n22   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n23   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n24   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n25   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n26   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n27   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n28   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n29   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n..        ...       ...   ...    ...      ...     ...       ...      ...   \n721  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n722  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n723  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n724  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n725  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n726  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n727  0.000000  0.391207   0.0    0.0      0.0     0.0       0.0      0.0   \n728  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n729  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n730  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n731  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n732  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n733  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n734  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n735  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n736  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n737  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n738  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n739  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n740  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n741  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n742  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n743  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n744  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n745  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n746  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n747  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n748  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n749  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n750  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n\n     andrew      anew   ...        yemen  yen  yield  york     young  younger  \\\n0       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n1       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n2       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n3       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n4       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n5       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n6       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n7       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n8       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n9       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n10      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.423583      0.0   \n11      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n12      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n13      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n14      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n15      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n16      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n17      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n18      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n19      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n20      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n21      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n22      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n23      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n24      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n25      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n26      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n27      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n28      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n29      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n..      ...       ...   ...          ...  ...    ...   ...       ...      ...   \n721     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n722     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n723     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n724     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n725     0.0  0.000000   ...     0.351819  0.0    0.0   0.0  0.000000      0.0   \n726     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n727     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n728     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n729     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n730     0.0  0.362871   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n731     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n732     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n733     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n734     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n735     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n736     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n737     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n738     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n739     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n740     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n741     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n742     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n743     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n744     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n745     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n746     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n747     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n748     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n749     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n750     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n\n        youth  youv    yuliya  zealand  \n0    0.000000   0.0  0.000000      0.0  \n1    0.000000   0.0  0.000000      0.0  \n2    0.000000   0.0  0.000000      0.0  \n3    0.000000   0.0  0.000000      0.0  \n4    0.000000   0.0  0.000000      0.0  \n5    0.000000   0.0  0.000000      0.0  \n6    0.000000   0.0  0.000000      0.0  \n7    0.000000   0.0  0.000000      0.0  \n8    0.000000   0.0  0.000000      0.0  \n9    0.000000   0.0  0.000000      0.0  \n10   0.000000   0.0  0.000000      0.0  \n11   0.000000   0.0  0.000000      0.0  \n12   0.000000   0.0  0.000000      0.0  \n13   0.000000   0.0  0.000000      0.0  \n14   0.000000   0.0  0.000000      0.0  \n15   0.000000   0.0  0.000000      0.0  \n16   0.000000   0.0  0.000000      0.0  \n17   0.000000   0.0  0.000000      0.0  \n18   0.000000   0.0  0.000000      0.0  \n19   0.000000   0.0  0.000000      0.0  \n20   0.000000   0.0  0.000000      0.0  \n21   0.000000   0.0  0.000000      0.0  \n22   0.000000   0.0  0.000000      0.0  \n23   0.000000   0.0  0.000000      0.0  \n24   0.000000   0.0  0.000000      0.0  \n25   0.000000   0.0  0.000000      0.0  \n26   0.000000   0.0  0.000000      0.0  \n27   0.000000   0.0  0.000000      0.0  \n28   0.000000   0.0  0.000000      0.0  \n29   0.000000   0.0  0.000000      0.0  \n..        ...   ...       ...      ...  \n721  0.000000   0.0  0.000000      0.0  \n722  0.000000   0.0  0.000000      0.0  \n723  0.000000   0.0  0.000000      0.0  \n724  0.000000   0.0  0.000000      0.0  \n725  0.000000   0.0  0.000000      0.0  \n726  0.000000   0.0  0.000000      0.0  \n727  0.000000   0.0  0.000000      0.0  \n728  0.000000   0.0  0.000000      0.0  \n729  0.414721   0.0  0.000000      0.0  \n730  0.000000   0.0  0.362871      0.0  \n731  0.000000   0.0  0.000000      0.0  \n732  0.000000   0.0  0.000000      0.0  \n733  0.000000   0.0  0.000000      0.0  \n734  0.000000   0.0  0.000000      0.0  \n735  0.000000   0.0  0.000000      0.0  \n736  0.000000   0.0  0.000000      0.0  \n737  0.000000   0.0  0.000000      0.0  \n738  0.000000   0.0  0.000000      0.0  \n739  0.000000   0.0  0.000000      0.0  \n740  0.000000   0.0  0.000000      0.0  \n741  0.000000   0.0  0.000000      0.0  \n742  0.000000   0.0  0.000000      0.0  \n743  0.000000   0.0  0.000000      0.0  \n744  0.000000   0.0  0.000000      0.0  \n745  0.000000   0.0  0.000000      0.0  \n746  0.000000   0.0  0.000000      0.0  \n747  0.000000   0.0  0.000000      0.0  \n748  0.000000   0.0  0.000000      0.0  \n749  0.000000   0.0  0.000000      0.0  \n750  0.000000   0.0  0.000000      0.0  \n\n[751 rows x 2041 columns]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-07fefa19978f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make train/test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \"\"\"\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Don't get num_samples from an ensembles length!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise TypeError('Expected sequence or array-like, got '\n\u001b[0;32m--> 112\u001b[0;31m                         'estimator %s' % x)\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got estimator       america  american  amid  amour  amplifi  analog  ancestor  ancient  \\\n0    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n1    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n2    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n3    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n4    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n5    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n6    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n7    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n8    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n9    0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n10   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n11   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n12   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n13   0.361474  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n14   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n15   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n16   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n17   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n18   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n19   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n20   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n21   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n22   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n23   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n24   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n25   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n26   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n27   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n28   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n29   0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n..        ...       ...   ...    ...      ...     ...       ...      ...   \n721  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n722  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n723  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n724  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n725  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n726  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n727  0.000000  0.391207   0.0    0.0      0.0     0.0       0.0      0.0   \n728  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n729  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n730  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n731  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n732  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n733  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n734  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n735  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n736  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n737  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n738  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n739  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n740  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n741  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n742  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n743  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n744  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n745  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n746  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n747  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n748  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n749  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n750  0.000000  0.000000   0.0    0.0      0.0     0.0       0.0      0.0   \n\n     andrew      anew   ...        yemen  yen  yield  york     young  younger  \\\n0       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n1       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n2       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n3       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n4       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n5       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n6       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n7       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n8       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n9       0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n10      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.423583      0.0   \n11      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n12      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n13      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n14      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n15      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n16      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n17      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n18      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n19      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n20      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n21      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n22      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n23      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n24      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n25      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n26      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n27      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n28      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n29      0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n..      ...       ...   ...          ...  ...    ...   ...       ...      ...   \n721     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n722     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n723     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n724     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n725     0.0  0.000000   ...     0.351819  0.0    0.0   0.0  0.000000      0.0   \n726     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n727     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n728     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n729     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n730     0.0  0.362871   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n731     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n732     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n733     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n734     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n735     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n736     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n737     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n738     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n739     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n740     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n741     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n742     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n743     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n744     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n745     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n746     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n747     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n748     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n749     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n750     0.0  0.000000   ...     0.000000  0.0    0.0   0.0  0.000000      0.0   \n\n        youth  youv    yuliya  zealand  \n0    0.000000   0.0  0.000000      0.0  \n1    0.000000   0.0  0.000000      0.0  \n2    0.000000   0.0  0.000000      0.0  \n3    0.000000   0.0  0.000000      0.0  \n4    0.000000   0.0  0.000000      0.0  \n5    0.000000   0.0  0.000000      0.0  \n6    0.000000   0.0  0.000000      0.0  \n7    0.000000   0.0  0.000000      0.0  \n8    0.000000   0.0  0.000000      0.0  \n9    0.000000   0.0  0.000000      0.0  \n10   0.000000   0.0  0.000000      0.0  \n11   0.000000   0.0  0.000000      0.0  \n12   0.000000   0.0  0.000000      0.0  \n13   0.000000   0.0  0.000000      0.0  \n14   0.000000   0.0  0.000000      0.0  \n15   0.000000   0.0  0.000000      0.0  \n16   0.000000   0.0  0.000000      0.0  \n17   0.000000   0.0  0.000000      0.0  \n18   0.000000   0.0  0.000000      0.0  \n19   0.000000   0.0  0.000000      0.0  \n20   0.000000   0.0  0.000000      0.0  \n21   0.000000   0.0  0.000000      0.0  \n22   0.000000   0.0  0.000000      0.0  \n23   0.000000   0.0  0.000000      0.0  \n24   0.000000   0.0  0.000000      0.0  \n25   0.000000   0.0  0.000000      0.0  \n26   0.000000   0.0  0.000000      0.0  \n27   0.000000   0.0  0.000000      0.0  \n28   0.000000   0.0  0.000000      0.0  \n29   0.000000   0.0  0.000000      0.0  \n..        ...   ...       ...      ...  \n721  0.000000   0.0  0.000000      0.0  \n722  0.000000   0.0  0.000000      0.0  \n723  0.000000   0.0  0.000000      0.0  \n724  0.000000   0.0  0.000000      0.0  \n725  0.000000   0.0  0.000000      0.0  \n726  0.000000   0.0  0.000000      0.0  \n727  0.000000   0.0  0.000000      0.0  \n728  0.000000   0.0  0.000000      0.0  \n729  0.414721   0.0  0.000000      0.0  \n730  0.000000   0.0  0.362871      0.0  \n731  0.000000   0.0  0.000000      0.0  \n732  0.000000   0.0  0.000000      0.0  \n733  0.000000   0.0  0.000000      0.0  \n734  0.000000   0.0  0.000000      0.0  \n735  0.000000   0.0  0.000000      0.0  \n736  0.000000   0.0  0.000000      0.0  \n737  0.000000   0.0  0.000000      0.0  \n738  0.000000   0.0  0.000000      0.0  \n739  0.000000   0.0  0.000000      0.0  \n740  0.000000   0.0  0.000000      0.0  \n741  0.000000   0.0  0.000000      0.0  \n742  0.000000   0.0  0.000000      0.0  \n743  0.000000   0.0  0.000000      0.0  \n744  0.000000   0.0  0.000000      0.0  \n745  0.000000   0.0  0.000000      0.0  \n746  0.000000   0.0  0.000000      0.0  \n747  0.000000   0.0  0.000000      0.0  \n748  0.000000   0.0  0.000000      0.0  \n749  0.000000   0.0  0.000000      0.0  \n750  0.000000   0.0  0.000000      0.0  \n\n[751 rows x 2041 columns]"
     ]
    }
   ],
   "source": [
    "# make train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit decision tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "score = np.mean(cross_val_score(dt, X_test, y_test, cv = 5, scoring = \"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 80% of the articles are opinion\n",
    "1- 58.0/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing titles for NLTK Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make these functions part of the list of words for documents\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        if stemmer.stem(item) not in nltk.corpus.stopwords.words('english'):\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.encode('ascii', errors = 'replace')\n",
    "    text = ''.join([ch.lower() for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing for NLTK Naive Bayes\n",
    "# titles have been tokenized, accents/punctuation/stopwords removed\n",
    "#documents = [(tokenize(title), label) for title in data['title'] for label in y]\n",
    "labels = [\"opinion\" if i == 'opinion' else \"not opinion\" for i in data['section']]\n",
    "tokenized_titles = [tokenize(title) for title in data['title']]\n",
    "documents = zip(tokenized_titles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a giant list of all the words in the titles\n",
    "word_list = []\n",
    "for title in data['title']:\n",
    "    word_list.extend(tokenize(title))\n",
    "all_words = nltk.FreqDist(w.lower() for w in word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'trump', 156), (u'new', 37), (u'ban', 33), (u'say', 32), (u'immigr', 28)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_features(doc):\n",
    "    doc_words = set(doc)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in doc_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_sets = [(doc_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split into train and tests\n",
    "train_set, test_set = feature_sets[90:], feature_sets[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655555555556\n"
     ]
    }
   ],
   "source": [
    "# test it on test set\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  donald = True           opinio : not op =     17.1 : 1.0\n",
      "                    dont = True           opinio : not op =     11.7 : 1.0\n",
      "                   fraud = True           opinio : not op =     11.7 : 1.0\n",
      "                   march = True           opinio : not op =      8.4 : 1.0\n",
      "                    moor = True           opinio : not op =      8.4 : 1.0\n",
      "                    neil = True           opinio : not op =      7.9 : 1.0\n",
      "                  nomine = True           opinio : not op =      7.0 : 1.0\n",
      "                    hate = True           opinio : not op =      5.0 : 1.0\n",
      "                  outcri = True           opinio : not op =      5.0 : 1.0\n",
      "                     way = True           opinio : not op =      5.0 : 1.0\n",
      "                 assault = True           opinio : not op =      5.0 : 1.0\n",
      "                  harlem = True           opinio : not op =      5.0 : 1.0\n",
      "                   thoma = True           opinio : not op =      5.0 : 1.0\n",
      "                     ten = True           opinio : not op =      5.0 : 1.0\n",
      "                    vote = True           opinio : not op =      5.0 : 1.0\n",
      "                    idea = True           opinio : not op =      5.0 : 1.0\n",
      "                   sorri = True           opinio : not op =      5.0 : 1.0\n",
      "                      mr = True           opinio : not op =      5.0 : 1.0\n",
      "                 inquiri = True           opinio : not op =      5.0 : 1.0\n",
      "                    shot = True           opinio : not op =      5.0 : 1.0\n",
      "                    mari = True           opinio : not op =      5.0 : 1.0\n",
      "                   liber = True           opinio : not op =      5.0 : 1.0\n",
      "                    deal = True           opinio : not op =      5.0 : 1.0\n",
      "                  russia = True           opinio : not op =      5.0 : 1.0\n",
      "                    nurs = True           opinio : not op =      5.0 : 1.0\n",
      "                  scalia = True           opinio : not op =      5.0 : 1.0\n",
      "                  danger = True           opinio : not op =      5.0 : 1.0\n",
      "             uncertainti = True           opinio : not op =      5.0 : 1.0\n",
      "                  prison = True           opinio : not op =      5.0 : 1.0\n",
      "                    voic = True           opinio : not op =      5.0 : 1.0\n",
      "                doomsday = True           opinio : not op =      5.0 : 1.0\n",
      "                     odd = True           opinio : not op =      5.0 : 1.0\n",
      "                   egypt = True           opinio : not op =      5.0 : 1.0\n",
      "                  stolen = True           opinio : not op =      5.0 : 1.0\n",
      "                     lie = True           opinio : not op =      5.0 : 1.0\n",
      "                  advanc = True           opinio : not op =      5.0 : 1.0\n",
      "                  bannon = True           opinio : not op =      5.0 : 1.0\n",
      "                  ranger = True           opinio : not op =      5.0 : 1.0\n",
      "                  disord = True           opinio : not op =      5.0 : 1.0\n",
      "                    save = True           opinio : not op =      5.0 : 1.0\n",
      "                 hardbal = True           opinio : not op =      5.0 : 1.0\n",
      "                    forc = True           opinio : not op =      5.0 : 1.0\n",
      "                  border = True           opinio : not op =      5.0 : 1.0\n",
      "                 fantasi = True           opinio : not op =      5.0 : 1.0\n",
      "                    seat = True           opinio : not op =      5.0 : 1.0\n",
      "                   speak = True           opinio : not op =      5.0 : 1.0\n",
      "                  symbol = True           opinio : not op =      5.0 : 1.0\n",
      "                  search = True           opinio : not op =      5.0 : 1.0\n",
      "                  suprem = True           opinio : not op =      5.0 : 1.0\n",
      "                 gorsuch = True           opinio : not op =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# get most informative features\n",
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stopwords = None, punct = None, lower = True, strip = True):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.punct = set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    def transform(self, X):\n",
    "        return[\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "    def tokenize(self, document):\n",
    "        document = document.encode('ascii', errors = 'replace')\n",
    "        #document = ''.join([ch.lower() for ch in text if ch not in string.punctuation])\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)): \n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_and_evaluate(X, y, \n",
    "                       classifier = MultinomialNB, outpath = None, verbose = True):\n",
    "    def build(classifier, X, y=None):\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "            \n",
    "        model = Pipeline([\n",
    "                ('preprocessor', NLTKPreprocessor()),\n",
    "                ('vectorizer', TfidfVectorizer(\n",
    "                    tokenizer = identity, preprocessor=None, \n",
    "                        lowercase = False)),\n",
    "                ('classifier', classifier)\n",
    "            ])\n",
    "\n",
    "        model.fit(X,y)\n",
    "        return model\n",
    "    \n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "    \n",
    "    if verbose: \n",
    "        print \"Building for evaluation\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    model = build(classifier, X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print \"Classification Report: \\n\"\n",
    "        \n",
    "    y_pred = model.predict(X_test)\n",
    "    print classification_report(y_test, y_pred, target_names = labels.classes_)\n",
    "    \n",
    "    if verbose:\n",
    "        print \"Building complete model and saving ...\"\n",
    "        \n",
    "    model = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "    \n",
    "    print \"Done\"\n",
    "    \n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {}.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "\n",
    "    # Get the top n and bottom n coef, name pairs\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\n",
    "            \"Classified as: {}\".format(model.predict([text]))\n",
    "        )\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(\n",
    "                cp, fnp, cn, fnn\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n",
      "Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "non-opinion       0.86      1.00      0.93       130\n",
      "    opinion       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.74      0.86      0.80       151\n",
      "\n",
      "Building complete model and saving ...\n"
     ]
    }
   ],
   "source": [
    "X = data['body']\n",
    "y = ['opinion' if x == 'opinion' else 'non-opinion' for x in data['section']]\n",
    "model = build_and_evaluate(X, y, outpath = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n",
      "Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "non-opinion       0.78      1.00      0.88       118\n",
      "    opinion       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.61      0.78      0.69       151\n",
      "\n",
      "Building complete model and saving ...\n",
      "Done\n",
      "Model written out to /Users/teresaborcuch/capstone_project/model.pkl\n"
     ]
    }
   ],
   "source": [
    "X = data['title']\n",
    "y = ['opinion' if x == 'opinion' else 'non-opinion' for x in data['section']]\n",
    "model = build_and_evaluate(X, y, outpath = '/Users/teresaborcuch/capstone_project/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-a3e386d2b845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-193-89ac6f31f547>\u001b[0m in \u001b[0;36mshow_most_informative_features\u001b[0;34m(model, text, n)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Compute the coefficients for the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Otherwise simply use the coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# delegate only on instances, not the classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# this is to allow access to the docstrings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "with open('/Users/teresaborcuch/capstone_project/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "show_most_informative_features(model, text = data['title'], n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on Article Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308921438082557"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 83% of the articles are not opinion\n",
    "1 - 127.0/751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing for NLTK Naive Bayes\n",
    "# titles have been tokenized, accents/punctuation/stopwords removed\n",
    "#documents = [(tokenize(title), label) for title in data['title'] for label in y]\n",
    "labels = [\"opinion\" if i == 'opinion' else \"not opinion\" for i in data['section']]\n",
    "tokenized_articles = [tokenize(body) for body in data['body']]\n",
    "documents = zip(tokenized_articles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a giant list of all the words in the articles\n",
    "word_list = []\n",
    "for title in data['body']:\n",
    "    word_list.extend(tokenize(title))\n",
    "all_words = nltk.FreqDist(w.lower() for w in word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wa', 4947), (u'said', 4379), (u'mr', 4066), (u'hi', 3405), (u'ha', 2878)]\n"
     ]
    }
   ],
   "source": [
    "print all_words.most_common(5)\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = [(doc_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and tests\n",
    "train_set, test_set = feature_sets[90:], feature_sets[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722222222222\n"
     ]
    }
   ],
   "source": [
    "# test it on test set\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        japaneseamerican = True           opinio : not op =      9.0 : 1.0\n",
      "          counterproduct = True           opinio : not op =      8.4 : 1.0\n",
      "                    tame = True           opinio : not op =      8.4 : 1.0\n",
      "                   troll = True           opinio : not op =      7.0 : 1.0\n",
      "                 prosper = True           opinio : not op =      6.5 : 1.0\n",
      "                selfserv = True           opinio : not op =      5.0 : 1.0\n",
      "             assemblyman = True           opinio : not op =      5.0 : 1.0\n",
      "                   intak = True           opinio : not op =      5.0 : 1.0\n",
      "                 perpetr = True           opinio : not op =      5.0 : 1.0\n",
      "                 tyranni = True           opinio : not op =      5.0 : 1.0\n",
      "                   ulcer = True           opinio : not op =      5.0 : 1.0\n",
      "                 particl = True           opinio : not op =      5.0 : 1.0\n",
      "               honeymoon = True           opinio : not op =      5.0 : 1.0\n",
      "             anticorrupt = True           opinio : not op =      5.0 : 1.0\n",
      "                    doom = True           opinio : not op =      5.0 : 1.0\n",
      "            countriesnow = True           opinio : not op =      5.0 : 1.0\n",
      "                    tamp = True           opinio : not op =      5.0 : 1.0\n",
      "              indonesian = True           opinio : not op =      5.0 : 1.0\n",
      "                  scenic = True           opinio : not op =      5.0 : 1.0\n",
      "                  ivanka = True           opinio : not op =      5.0 : 1.0\n",
      "                 publica = True           opinio : not op =      5.0 : 1.0\n",
      "                   scold = True           opinio : not op =      5.0 : 1.0\n",
      "                tombston = True           opinio : not op =      5.0 : 1.0\n",
      "           asianamerican = True           opinio : not op =      5.0 : 1.0\n",
      "              offendedmr = True           opinio : not op =      5.0 : 1.0\n",
      "               blatantli = True           opinio : not op =      5.0 : 1.0\n",
      "                   mango = True           opinio : not op =      5.0 : 1.0\n",
      "              irreconcil = True           opinio : not op =      5.0 : 1.0\n",
      "                 gunshot = True           opinio : not op =      5.0 : 1.0\n",
      "                  analog = True           opinio : not op =      5.0 : 1.0\n",
      "                     fig = True           opinio : not op =      5.0 : 1.0\n",
      "                 disprov = True           opinio : not op =      5.0 : 1.0\n",
      "               catalonia = True           opinio : not op =      5.0 : 1.0\n",
      "                   modif = True           opinio : not op =      5.0 : 1.0\n",
      "                symmetri = True           opinio : not op =      5.0 : 1.0\n",
      "                    davo = True           opinio : not op =      5.0 : 1.0\n",
      "             sponsorship = True           opinio : not op =      5.0 : 1.0\n",
      "                   asham = True           opinio : not op =      5.0 : 1.0\n",
      "                    jerk = True           opinio : not op =      5.0 : 1.0\n",
      "               firebrand = True           opinio : not op =      5.0 : 1.0\n",
      "                  buffer = True           opinio : not op =      5.0 : 1.0\n",
      "               euphemist = True           opinio : not op =      5.0 : 1.0\n",
      "                  corker = True           opinio : not op =      5.0 : 1.0\n",
      "                 inscrib = True           opinio : not op =      5.0 : 1.0\n",
      "                 flirtat = True           opinio : not op =      5.0 : 1.0\n",
      "                presumpt = True           opinio : not op =      5.0 : 1.0\n",
      "                  eleven = True           opinio : not op =      5.0 : 1.0\n",
      "                  harbor = True           opinio : not op =      5.0 : 1.0\n",
      "                   inher = True           opinio : not op =      4.3 : 1.0\n",
      "              bureaucrat = True           opinio : not op =      4.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# get most informative features\n",
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$5 Million for a Super Bowl Ad. Another Millio...</td>\n",
       "      <td>20170129</td>\n",
       "      <td>Sapna Maheshwari</td>\n",
       "      <td>This month, Anheuser-Busch InBev hosted a doze...</td>\n",
       "      <td>http://www.nytimes.com/2017/01/29/business/5-m...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$60,000 in Tuition, and My Son Wants to Become...</td>\n",
       "      <td>20170112</td>\n",
       "      <td>Philip Galanes</td>\n",
       "      <td>My wife and I are spending a fortune to send o...</td>\n",
       "      <td>http://www.nytimes.com/2017/01/12/fashion/farm...</td>\n",
       "      <td>fashion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      date  \\\n",
       "0  $5 Million for a Super Bowl Ad. Another Millio...  20170129   \n",
       "1  $60,000 in Tuition, and My Son Wants to Become...  20170112   \n",
       "\n",
       "             author                                               body  \\\n",
       "0  Sapna Maheshwari  This month, Anheuser-Busch InBev hosted a doze...   \n",
       "1    Philip Galanes  My wife and I are spending a fortune to send o...   \n",
       "\n",
       "                                                link   section  \n",
       "0  http://www.nytimes.com/2017/01/29/business/5-m...  business  \n",
       "1  http://www.nytimes.com/2017/01/12/fashion/farm...   fashion  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make file of opinion titles\n",
    "f = open('/Users/teresaborcuch/capstone_project/opinion_titles.txt', 'w')\n",
    "for i, row in data.iterrows():\n",
    "    if row[5] == 'opinion':\n",
    "        f.write(row[0].encode('utf-8'))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make file of opinion bodies\n",
    "f = open('/Users/teresaborcuch/capstone_project/opinion_bodies.txt', 'w')\n",
    "for i, row in data.iterrows():\n",
    "    if row[5] == 'opinion':\n",
    "        f.write(row[3].encode('utf-8'))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "text = open('/Users/teresaborcuch/capstone_project/opinion_bodies.txt').read()\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('/Users/teresaborcuch/capstone_project/opinion__body_big.png')\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(max_font_size = 40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('/Users/teresaborcuch/capstone_project/opinion_body_sm.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make file of non-opinion titles\n",
    "f = open('/Users/teresaborcuch/capstone_project/nonopinion_titles.txt', 'w')\n",
    "for i, row in data.iterrows():\n",
    "    if row[5] != 'opinion':\n",
    "        f.write(row[0].encode('utf-8'))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make file of non-opinion bodies\n",
    "f = open('/Users/teresaborcuch/capstone_project/nonopinion_bodies.txt', 'w')\n",
    "for i, row in data.iterrows():\n",
    "    if row[5] != 'opinion':\n",
    "        f.write(row[3].encode('utf-8'))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make non-opinion word cloud\n",
    "text = open('/Users/teresaborcuch/capstone_project/nonopinion_bodies.txt').read()\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('/Users/teresaborcuch/capstone_project/non_opinion_body_big.png')\n",
    "\n",
    "wordcloud = WordCloud(max_font_size = 40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('/Users/teresaborcuch/capstone_project/non_opinion_body_sm.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
