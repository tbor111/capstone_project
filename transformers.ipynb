{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import articledata\n",
    "from highchartsplotter import *\n",
    "import fitmodel\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, SelectFdr, chi2\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.corpus import names\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/Users/teresaborcuch/capstone_project/notebooks/final_data_test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return(X[self.key])\n",
    "    \n",
    "class BodyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "       \n",
    "        ss = [compute_score(x) for x in X['body']]\n",
    "        persons, places, males, females = count_entities(X['body'])\n",
    "        \n",
    "        body_array = map(list,(zip(persons, places, males, females,ss)))\n",
    "        return pd.DataFrame(body_array, columns = ['body_persons', 'body_places',\n",
    "                                                   'body_men', 'body_women', 'body_ss'])\n",
    "\n",
    "class TitleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        persons, places, males, females = count_entities(X['title'])\n",
    "        ss = np.asarray([compute_score(x) for x in X['title']])\n",
    "        \n",
    "        title_array = map(list,(zip(persons, places, males, females, ss))) #ss\n",
    "        \n",
    "        return pd.DataFrame(title_array, columns = ['title_persons', 'title_places', \n",
    "                                                    'title_men', 'title_women', 'title_ss'])\n",
    "    \n",
    "def count_entities(col):\n",
    "    # set up tagger\n",
    "    os.environ['CLASSPATH'] = \"/Users/teresaborcuch/stanford-ner-2013-11-12/stanford-ner.jar\"\n",
    "    os.environ['STANFORD_MODELS'] = '/Users/teresaborcuch/stanford-ner-2013-11-12/classifiers'\n",
    "    st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "    tagged_titles = []\n",
    "    persons = []\n",
    "    places = []\n",
    "    male_counts = []\n",
    "    female_counts = []\n",
    "    male_names = names.words(\"male.txt\")\n",
    "    female_names = names.words(\"female.txt\")\n",
    "\n",
    "    for x in col:\n",
    "        tokens = word_tokenize(x)\n",
    "        tags = st.tag(tokens)\n",
    "        tagged_titles.append(tags)\n",
    "\n",
    "    for pair_list in tagged_titles:\n",
    "        person_count = 0\n",
    "        place_count = 0\n",
    "\n",
    "        for pair in pair_list:\n",
    "            m_count = 0\n",
    "            f_count = 0\n",
    "            if pair[1] == 'PERSON':\n",
    "                person_count +=1\n",
    "                if pair[0] in male_names:\n",
    "                    m_count +=1\n",
    "                elif pair[0] in female_names:\n",
    "                    f_count +=1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "            elif pair[1] == 'LOCATION':\n",
    "                place_count +=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        persons.append(person_count)\n",
    "        places.append(place_count)\n",
    "        male_counts.append(m_count)\n",
    "        female_counts.append(f_count)\n",
    "        \n",
    "    return persons, places, male_counts, female_counts\n",
    "\n",
    "class DummyMaker(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dummies = pd.get_dummies(X)\n",
    "        return dummies\n",
    "    \n",
    "def compute_score(sentence):\n",
    "    tagger = PerceptronTagger()\n",
    "    taggedsentence = []\n",
    "    sent_score = []\n",
    "    taggedsentence.append(tagger.tag(sentence.split()))\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    for idx, words in enumerate(taggedsentence):\n",
    "        for idx2, t in enumerate(words):\n",
    "            newtag = ''\n",
    "            lemmatizedsent = wnl.lemmatize(t[0])\n",
    "            if t[1].startswith('NN'):\n",
    "                newtag = 'n'\n",
    "            elif t[1].startswith('JJ'):\n",
    "                newtag = 'a'\n",
    "            elif t[1].startswith('V'):\n",
    "                newtag = 'v'\n",
    "            elif t[1].startswith('R'):\n",
    "                newtag = 'r'\n",
    "            else:\n",
    "                newtag = ''\n",
    "            if (newtag != ''):\n",
    "                synsets = list(swn.senti_synsets(lemmatizedsent, newtag))\n",
    "                score = 0.0\n",
    "                if (len(synsets) > 0):\n",
    "                    for syn in synsets:\n",
    "                        score += syn.pos_score() - syn.neg_score()\n",
    "                    sent_score.append(score / len(synsets))\n",
    "        if (len(sent_score)==0 or len(sent_score)==1):\n",
    "            return (float(0.0))\n",
    "        else:\n",
    "            return (sum([word_score for word_score in sent_score]) / (len(sent_score)))\n",
    "\n",
    "class TfidfDF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary = None):\n",
    "        self.vocabulary = vocabulary\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tv = TfidfVectorizer(preprocessor = fitmodel.tokenize, ngram_range = (1,2), vocabulary = self.vocabulary)\n",
    "        tv.fit(X)\n",
    "        body_feat = tv.transform(X).todense()\n",
    "        df = pd.DataFrame(body_feat, columns = tv.get_feature_names())\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = TfidfDF().transform(test_data['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>000 cancer</th>\n",
       "      <th>000 case</th>\n",
       "      <th>000 death</th>\n",
       "      <th>000 people</th>\n",
       "      <th>000 year</th>\n",
       "      <th>10</th>\n",
       "      <th>10 percent</th>\n",
       "      <th>100</th>\n",
       "      <th>100 billion</th>\n",
       "      <th>...</th>\n",
       "      <th>young gay</th>\n",
       "      <th>youre</th>\n",
       "      <th>youre discussion</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube celebrate</th>\n",
       "      <th>youtube different</th>\n",
       "      <th>youtube see</th>\n",
       "      <th>youtube twitter</th>\n",
       "      <th>zamora</th>\n",
       "      <th>zamora mtv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024513</td>\n",
       "      <td>0.024513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071826</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.096577</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.044889</td>\n",
       "      <td>0.044889</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.026908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000  000 cancer  000 case  000 death  000 people  000 year        10  \\\n",
       "0  0.000000    0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
       "1  0.000000    0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
       "2  0.025926    0.000000  0.000000   0.000000    0.000000  0.032134  0.000000   \n",
       "3  0.096577    0.014963  0.044889   0.044889    0.014963  0.000000  0.014963   \n",
       "4  0.000000    0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
       "\n",
       "   10 percent       100  100 billion     ...      young gay     youre  \\\n",
       "0    0.000000  0.000000     0.000000     ...       0.000000  0.024513   \n",
       "1    0.000000  0.014487     0.000000     ...       0.000000  0.000000   \n",
       "2    0.000000  0.000000     0.000000     ...       0.000000  0.000000   \n",
       "3    0.014963  0.012072     0.014963     ...       0.000000  0.000000   \n",
       "4    0.000000  0.000000     0.000000     ...       0.026908  0.000000   \n",
       "\n",
       "   youre discussion   youtube  youtube celebrate  youtube different  \\\n",
       "0          0.024513  0.000000           0.000000           0.000000   \n",
       "1          0.000000  0.071826           0.017956           0.017956   \n",
       "2          0.000000  0.000000           0.000000           0.000000   \n",
       "3          0.000000  0.000000           0.000000           0.000000   \n",
       "4          0.000000  0.000000           0.000000           0.000000   \n",
       "\n",
       "   youtube see  youtube twitter    zamora  zamora mtv  \n",
       "0     0.000000         0.000000  0.000000    0.000000  \n",
       "1     0.017956         0.017956  0.000000    0.000000  \n",
       "2     0.000000         0.000000  0.000000    0.000000  \n",
       "3     0.000000         0.000000  0.000000    0.000000  \n",
       "4     0.000000         0.000000  0.026908    0.026908  \n",
       "\n",
       "[5 rows x 4414 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipe to train the model and establish the vocabulay\n",
    "training_pipe = Pipeline([('features', FeatureUnion([\n",
    "                                    ('titles', TitleTransformer()),\n",
    "                                    ('bodies', BodyTransformer()),\n",
    "                                    ('sources', Pipeline([('itemselector', ItemSelector(key = 'source')),\n",
    "                                                         ('dummy', DummyMaker())])),\n",
    "                                   ('tfidf-pipe', Pipeline([('itemselector', ItemSelector(key = 'body')),\n",
    "                                    ('tv', TfidfVectorizer(preprocessor = fitmodel.tokenize, \n",
    "                                                                           ngram_range = (1,2))),\n",
    "                                    ('dense', DenseTransformer()),\n",
    "                                    ('selector', SelectKBest(score_func = chi2, k = 5))]))\n",
    "                                  ])),\n",
    "                         ('scale', MinMaxScaler())\n",
    "                         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transformation pipe to transform new data\n",
    "transform_pipe = FeatureUnion([('titles', TitleTransformer()),\n",
    "                                    ('bodies', BodyTransformer()),\n",
    "                                    ('sources', Pipeline([('itemselector', ItemSelector(key = 'source')),\n",
    "                                                         ('dummy', DummyMaker())])),\n",
    "                                   ('tfidf-pipe', Pipeline([('itemselector', ItemSelector(key = 'body')),\n",
    "                                                              ('tfidf', TfidfDF())                     ]))\n",
    "                                  ])\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = test_data['condensed_section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'fitmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-75761fd99555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbody_feat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m             delayed(_fit_transform_one)(trans, name, X, y,\n\u001b[1;32m    496\u001b[0m                                         self.transformer_weights, **fit_params)\n\u001b[0;32m--> 497\u001b[0;31m             for name, trans in self.transformer_list)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, name, X, y, transformer_weights, **fit_params)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransformer_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/teresaborcuch/anaconda2/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ba820df15c8b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mbody_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'fitmodel' is not defined"
     ]
    }
   ],
   "source": [
    "body_feat2 = transform_pipe.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>000 2015</th>\n",
       "      <th>000 american</th>\n",
       "      <th>000 euro</th>\n",
       "      <th>000 job</th>\n",
       "      <th>000 member</th>\n",
       "      <th>000 mile</th>\n",
       "      <th>000 new</th>\n",
       "      <th>000 people</th>\n",
       "      <th>000 per</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000  000 2015  000 american  000 euro  000 job  000 member  000 mile  \\\n",
       "0  0.000000       0.0           0.0       0.0      0.0         0.0       0.0   \n",
       "1  0.000000       0.0           0.0       0.0      0.0         0.0       0.0   \n",
       "2  0.023754       0.0           0.0       0.0      0.0         0.0       0.0   \n",
       "3  0.072619       0.0           0.0       0.0      0.0         0.0       0.0   \n",
       "4  0.000000       0.0           0.0       0.0      0.0         0.0       0.0   \n",
       "\n",
       "   000 new  000 people  000 per     ...      zero  zhang  zimbabwe  zionist  \\\n",
       "0      0.0    0.000000      0.0     ...       0.0    0.0       0.0      0.0   \n",
       "1      0.0    0.000000      0.0     ...       0.0    0.0       0.0      0.0   \n",
       "2      0.0    0.000000      0.0     ...       0.0    0.0       0.0      0.0   \n",
       "3      0.0    0.015795      0.0     ...       0.0    0.0       0.0      0.0   \n",
       "4      0.0    0.000000      0.0     ...       0.0    0.0       0.0      0.0   \n",
       "\n",
       "   zip  zombie  zone  zoo  zoom  zuckerberg  \n",
       "0  0.0     0.0   0.0  0.0   0.0         0.0  \n",
       "1  0.0     0.0   0.0  0.0   0.0         0.0  \n",
       "2  0.0     0.0   0.0  0.0   0.0         0.0  \n",
       "3  0.0     0.0   0.0  0.0   0.0         0.0  \n",
       "4  0.0     0.0   0.0  0.0   0.0         0.0  \n",
       "\n",
       "[5 rows x 23182 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_feat2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get vocabulary to fit the permanent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(preprocessor = fitmodel.tokenize, ngram_range = (1,2), min_df = 10)\n",
    "tv.fit(data['body'])\n",
    "body_feat = pd.DataFrame(tv.transform(data['body']).todense(),\n",
    "                         columns = tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=5000, score_func=<function chi2 at 0x11a7ccaa0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['condensed_section']\n",
    "selector = SelectKBest(score_func = chi2, k = 5000)\n",
    "selector.fit(body_feat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = body_feat.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'000', u'000 people', u'000 student', u'000 year', u'05', u'06', u'10',\n",
       "       u'10 point', u'10 rebound', u'106',\n",
       "       ...\n",
       "       u'young people', u'young player', u'young son', u'youre', u'youtube',\n",
       "       u'zakaria', u'zealand', u'zimbabwe', u'zoo', u'zuckerberg'],\n",
       "      dtype='object', length=5000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try from import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import names\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk import WordNetLemmatizer, wordpunct_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "\n",
    "class ArticleData():\n",
    "\n",
    "    '''\n",
    "    Usage:\n",
    "    >>> from articledata import *\n",
    "    >>> data = ArticleData().call()\n",
    "    >>> data = get_sent_scores(data = data)\n",
    "    >>> topic_data = evaluate_topic(data = data, section = 'opinion', source = 'NYT', topic = 'healthcare')\n",
    "    >>> data = count_entities(data = data, title = True)\n",
    "    >>> people_dict, place_dict = evaluate_entities(data = data, section = 'opinion', source = 'NYT')\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def call(self):\n",
    "        engine = create_engine('postgresql://teresaborcuch@localhost:5433/capstone')\n",
    "\n",
    "        # get data from all three tables and add source column\n",
    "        query1 = \"SELECT DISTINCT ON(title) title, date, body, section FROM fox_news;\"\n",
    "        fox_data = pd.read_sql(query1, engine)\n",
    "        fox_data['source'] = ['Fox']*len(fox_data)\n",
    "\n",
    "        query2 = \"SELECT DISTINCT ON(title) title, date, body, section FROM ny_times;\"\n",
    "        nyt_data = pd.read_sql(query2, engine)\n",
    "        nyt_data['source'] = ['NYT'] * len(nyt_data)\n",
    "\n",
    "        query3 = \"SELECT DISTINCT ON(title) title, date, body, section FROM washington_post;\"\n",
    "        wp_data = pd.read_sql(query3, engine)\n",
    "        wp_data['source'] = ['WP']*len(wp_data)\n",
    "\n",
    "        # merge the dataframes into one big one\n",
    "        data = pd.concat([nyt_data, fox_data, wp_data], axis = 0)\n",
    "\n",
    "\n",
    "        # drop those with empty or suspiciously short bodies\n",
    "        problem_rows = []\n",
    "        for i, row in data.iterrows():\n",
    "            try:\n",
    "                if len(row[2]) < 200:\n",
    "                    problem_rows.append(row.name)\n",
    "            except TypeError:\n",
    "                problem_rows.append(row.name)\n",
    "\n",
    "        data = data.drop(data.index[problem_rows])\n",
    "\n",
    "        # fix the dates\n",
    "        new_dates = []\n",
    "        for x in data['date']:\n",
    "            if type(x) == int:\n",
    "                x = str(x)\n",
    "                x = (x[:4] + '-' + x[4:6] + '-' + x[6:8]).replace(' 00:00:00','')\n",
    "                x = datetime.strptime(x, '%Y-%m-%d')\n",
    "                new_dates.append(x)\n",
    "            else:\n",
    "                x = str(x).replace(' 00:00:00','')\n",
    "                x = datetime.strptime(x, '%Y-%m-%d')\n",
    "                new_dates.append(x)\n",
    "\n",
    "        data['date'] = new_dates\n",
    "\n",
    "        # eliminate | Fox News from titles\n",
    "        clean_titles = []\n",
    "        for x in data['title']:\n",
    "            match = re.search('\\|.*$', x)\n",
    "            if match:\n",
    "                clean_x = re.sub('\\|.*$','',x)\n",
    "                clean_titles.append(clean_x)\n",
    "            else:\n",
    "                clean_titles.append(x)\n",
    "        data['title'] = clean_titles\n",
    "\n",
    "\n",
    "        # create the condensed section\n",
    "        def condense_section(x):\n",
    "            if 'world' in x:\n",
    "                section = 'world'\n",
    "            elif 'pinion' in x:\n",
    "                section = 'opinion'\n",
    "            elif ('business' in x) or ('tech' in x):\n",
    "                section = 'bus_tech'\n",
    "            elif ('entertain' in x) or ('art' in x) or ('theater' in x) or ('book' in x) or ('movie' in x) or ('travel' in x) or ('fashion' in x) or ('style' in x) or ('dining' in x):\n",
    "                section = 'entertainment'\n",
    "            elif 'sport' in x:\n",
    "                section = 'sports'\n",
    "            elif ('health' in x) or ('science' in x) or ('well' in x):\n",
    "                section = 'sci_health'\n",
    "            elif ('education' in x):\n",
    "                section = 'education'\n",
    "            elif ('olitic' in x) or ('us' in x) or ('national' in x) or ('powerpost' in x):\n",
    "                section = 'politics'\n",
    "            else:\n",
    "                section = 'other'\n",
    "            return section\n",
    "\n",
    "        data['condensed_section'] = [condense_section(x) for x in data['section']]\n",
    "        data = data.reset_index(drop = True)\n",
    "        mask1 = data['condensed_section'] != 'other'\n",
    "        mask2 = data['condensed_section'] != 'education'\n",
    "        data = data[mask1 & mask2]\n",
    "\n",
    "        return data\n",
    "\n",
    "class EvaluateTime():\n",
    "    '''\n",
    "    Usage:\n",
    "    >>> et = EvaluateTime(data = data, source = 'NYT', section = 'politics', topic = 'health')\n",
    "    >>> et.plot_time()\n",
    "    '''\n",
    "    def __init__(self, data = None, section = None, source = None, topic = None, date = None):\n",
    "        self.data = data\n",
    "        self.section = section\n",
    "        self.source = source\n",
    "        self.topic = topic\n",
    "        self.date = date\n",
    "\n",
    "    def call(self):\n",
    "        #self.plot_date_dict,\n",
    "        self.range_date_dict, self.groupings = self.make_dict()\n",
    "        return self\n",
    "\n",
    "    def make_dict(self):\n",
    "        # define masks\n",
    "        section_mask = (self.data['condensed_section'] == self.section)\n",
    "        source_mask = (self.data['source'] == self.source)\n",
    "        date_mask = (self.data['date'] > self.date)\n",
    "\n",
    "        # initialize lists for plot_date_dict\n",
    "        topic_scores = []\n",
    "        dates = []\n",
    "        groupings = []\n",
    "\n",
    "        # initialize other dict\n",
    "        range_date_dict = {}\n",
    "\n",
    "        if not self.date:\n",
    "            print \"Please select a start date.\"\n",
    "\n",
    "        # make plot_date_dict from appropriate subset of data\n",
    "        else:\n",
    "            if self.section and self.source:\n",
    "                masked_data = self.data[section_mask & source_mask & date_mask]\n",
    "\n",
    "            elif self.section and (not self.source):\n",
    "                masked_data = self.data[section_mask & date_mask]\n",
    "\n",
    "            elif self.source and (not self.section):\n",
    "                masked_data = self.data[source_mask & date_mask]\n",
    "\n",
    "            else:\n",
    "                masked_data = self.data[date_mask]\n",
    "\n",
    "            for i, row in masked_data.iterrows():\n",
    "\n",
    "                if self.topic in row[2]:\n",
    "                    topic_scores.append(row[6]) #body score\n",
    "                    dates.append(row[1])\n",
    "                    score_title_date = (row[0], row[1], row[6])\n",
    "                    groupings.append(score_title_date)\n",
    "\n",
    "\n",
    "                    # add to range_date_dict where keys are the dates and the values are a list of scores\n",
    "                    if row[1] not in range_date_dict.keys():\n",
    "                        range_date_dict[row[1]] = [row[6]]\n",
    "\n",
    "                    elif row[1] in range_date_dict.keys():\n",
    "                        (range_date_dict[row[1]]).append(row[6])\n",
    "\n",
    "        return range_date_dict, groupings\n",
    "\n",
    "\n",
    "    def plot_time(self):\n",
    "\n",
    "        x = self.range_date_dict.keys()\n",
    "        x.sort()\n",
    "        ordered_x = []\n",
    "        y = []\n",
    "        for val in x:\n",
    "            ordered_x.append(val)\n",
    "            values = self.range_date_dict[val]\n",
    "            mean = np.mean(values)\n",
    "            y.append(mean)\n",
    "\n",
    "        # define upper and lower boundaries for error bars\n",
    "        upper_bounds = [max(self.range_date_dict[x]) for x in ordered_x]\n",
    "        lower_bounds = [min(self.range_date_dict[x]) for x in ordered_x]\n",
    "\n",
    "        # define distance for upper error bar\n",
    "        y_upper = zip(y, upper_bounds)\n",
    "        upper_error = [abs(pair[0] - pair[1]) for pair in y_upper]\n",
    "\n",
    "        # define distance for lower error bar\n",
    "        y_lower = zip(y, lower_bounds)\n",
    "        lower_error = [abs(pair[0] - pair[1]) for pair in y_lower]\n",
    "\n",
    "        asymmetric_error = [lower_error, upper_error]\n",
    "\n",
    "        plt.plot(ordered_x, y, c = 'r', marker = 'o')\n",
    "        plt.errorbar(ordered_x, y, yerr = asymmetric_error, ecolor = 'r', capthick = 1)\n",
    "        plt.xlim(min(ordered_x) + timedelta(days = -1), max(ordered_x) + timedelta(days = 1))\n",
    "        plt.xticks(rotation = 70)\n",
    "        plt.show()\n",
    "\n",
    "class HighChartPlotter():\n",
    "    def __init__(self, et):\n",
    "        self.et = et\n",
    "\n",
    "    def call(self):\n",
    "\n",
    "        self.x_dates, self.y_means, self.error_pairs, self.date_list = self.get_plotting_data()\n",
    "\n",
    "        self.groups = self.et.groupings\n",
    "\n",
    "        self.spline_series = self.get_spline_series()\n",
    "\n",
    "        self.error_bar_series = self.get_error_bar_series()\n",
    "\n",
    "        self.min_titles, self.max_titles = self.get_titles()\n",
    "\n",
    "        self.min_scatter_series, self.max_scatter_series = self.get_scatter_points()\n",
    "\n",
    "\n",
    "    def get_plotting_data(self):\n",
    "        # get dates for x-axis\n",
    "        date_list = self.et.range_date_dict.keys()\n",
    "        date_list.sort()\n",
    "        x_dates = [x.value// 10 ** 6 for x in date_list]\n",
    "\n",
    "        # y-values\n",
    "        y_values = [np.mean(self.et.range_date_dict[x]) for x in date_list]\n",
    "\n",
    "        # error bars\n",
    "        error_min_max = []\n",
    "        for x in date_list:\n",
    "            temp_list = []\n",
    "            minimum = min(self.et.range_date_dict[x])\n",
    "            maximum = max(self.et.range_date_dict[x])\n",
    "            temp_list.append(minimum)\n",
    "            temp_list.append(maximum)\n",
    "            error_min_max.append(temp_list)\n",
    "\n",
    "        return x_dates, y_values, error_min_max, date_list\n",
    "\n",
    "    def get_spline_series(self):\n",
    "        # format splines for jsfiddle - do this first!\n",
    "        d = []\n",
    "        series = {'name': 'Mean Score', 'type': 'spline'}\n",
    "        for x in range(len(self.date_list)):\n",
    "            data_point = [self.x_dates[x], self.y_means[x]]\n",
    "            d.append(data_point)\n",
    "        series['data'] = d\n",
    "        spline_series = json.dumps(series)\n",
    "        return spline_series\n",
    "\n",
    "    def get_error_bar_series(self):\n",
    "        d = []\n",
    "        series = {'color': '#FF0000', 'name': 'Range', 'type': 'errorbar', 'stemWidth': 3, 'whiskerLength': 0}\n",
    "        for x in range(len(self.date_list)):\n",
    "            data_point = [self.x_dates[x], self.error_pairs[x][0], self.error_pairs[x][1]]\n",
    "            d.append(data_point)\n",
    "        series['data'] = d\n",
    "        error_series = json.dumps(series)\n",
    "        return error_series\n",
    "\n",
    "    def get_titles(self):\n",
    "        min_score_titles = {}\n",
    "        max_score_titles = {}\n",
    "\n",
    "        # min scores\n",
    "        for x in self.groups:\n",
    "            if x[1] not in min_score_titles.keys():\n",
    "                min_score_titles[x[1]] = (x[2], x[0])\n",
    "            elif x[1] in min_score_titles.keys():\n",
    "                if x[2] < min_score_titles[x[1]][0]:\n",
    "                    min_score_titles[x[1]] = (x[2], x[0])\n",
    "                elif x[2] >= min_score_titles[x[1]][0]:\n",
    "                    continue\n",
    "\n",
    "        # max scores\n",
    "        for x in self.groups:\n",
    "            if x[1] not in max_score_titles.keys():\n",
    "                max_score_titles[x[1]] = (x[2], x[0])\n",
    "            elif x[1] in max_score_titles.keys():\n",
    "                if x[2] > max_score_titles[x[1]][0]:\n",
    "                    max_score_titles[x[1]] = (x[2], x[0])\n",
    "                elif x[2] <= max_score_titles[x[1]][0]:\n",
    "                    continue\n",
    "\n",
    "        min_titles = [min_score_titles[x][1].encode('ascii', 'ignore') for x in self.date_list]\n",
    "        max_titles = [max_score_titles[x][1].encode('ascii','ignore') for x in self.date_list]\n",
    "\n",
    "        return min_titles, max_titles\n",
    "\n",
    "    def get_scatter_points(self):\n",
    "        max_series = []\n",
    "\n",
    "        for x in range(len(self.date_list)):\n",
    "            data_point = {'showInLegend': False, 'type': 'scatter', 'color': '#FF0000',\n",
    "                          'marker': {'symbol': 'circle', 'enabled': True, 'color': '#FF0000'},\n",
    "                          'tooltip': {'pointFormat': '{point.y}'}}\n",
    "\n",
    "            data_point['name'] = self.max_titles[x]\n",
    "\n",
    "            data_list = [[self.x_dates[x], self.error_pairs[x][1]]]\n",
    "\n",
    "            data_point['data'] = data_list\n",
    "            max_series.append(data_point)\n",
    "\n",
    "        max_series = json.dumps(max_series)\n",
    "\n",
    "        # return minimum scatter points series\n",
    "        min_series = []\n",
    "\n",
    "        for x in range(len(self.date_list)):\n",
    "            data_point = {'showInLegend': False, 'type': 'scatter', 'color': '#FF0000',\n",
    "                          'marker': {'symbol': 'circle', 'enabled': True, 'color': '#FF0000'},\n",
    "                          'tooltip': {'pointFormat': '{point.y}'}}\n",
    "            # get title\n",
    "            data_point['name'] = self.min_titles[x]\n",
    "\n",
    "            data_list = [[self.x_dates[x], self.error_pairs[x][0]]]\n",
    "\n",
    "            data_point['data'] = data_list\n",
    "\n",
    "            min_series.append(data_point)\n",
    "\n",
    "        min_series = json.dumps(min_series)\n",
    "\n",
    "        return min_series, max_series\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return(X[self.key])\n",
    "\n",
    "class BodyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        ss = [compute_score(x) for x in X['body']]\n",
    "        persons, places, males, females = count_entities(X['body'])\n",
    "\n",
    "        body_array = map(list,(zip(persons, places, males, females,ss)))\n",
    "        return pd.DataFrame(body_array, columns = ['body_persons', 'body_places',\n",
    "                                                   'body_men', 'body_women', 'body_ss'])\n",
    "    def get_feature_names(self):\n",
    "        return ['body_persons', 'body_places', 'body_men', 'body_women', 'body_ss']\n",
    "\n",
    "class TitleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        persons, places, males, females = count_entities(X['title'])\n",
    "        ss = np.asarray([compute_score(x) for x in X['title']])\n",
    "        \n",
    "        title_array = map(list,(zip(persons, places, males, females, ss))) #ss\n",
    "\n",
    "        return pd.DataFrame(title_array, columns = ['title_persons', 'title_places',\n",
    "                                                    'title_men', 'title_women', 'title_ss'])\n",
    "    def get_feature_names(self):\n",
    "        return ['title_persons', 'title_places','title_men', 'title_women', 'title_ss']\n",
    "\n",
    "class DummyMaker(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.dummies = pd.get_dummies(X)\n",
    "        return self.dummies\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.dummies.columns\n",
    "\n",
    "def compute_score(sentence):\n",
    "    tagger = PerceptronTagger()\n",
    "    taggedsentence = []\n",
    "    sent_score = []\n",
    "    taggedsentence.append(tagger.tag(sentence.split()))\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    for idx, words in enumerate(taggedsentence):\n",
    "        for idx2, t in enumerate(words):\n",
    "            newtag = ''\n",
    "            lemmatizedsent = wnl.lemmatize(t[0])\n",
    "            if t[1].startswith('NN'):\n",
    "                newtag = 'n'\n",
    "            elif t[1].startswith('JJ'):\n",
    "                newtag = 'a'\n",
    "            elif t[1].startswith('V'):\n",
    "                newtag = 'v'\n",
    "            elif t[1].startswith('R'):\n",
    "                newtag = 'r'\n",
    "            else:\n",
    "                newtag = ''\n",
    "            if (newtag != ''):\n",
    "                synsets = list(swn.senti_synsets(lemmatizedsent, newtag))\n",
    "                score = 0.0\n",
    "                if (len(synsets) > 0):\n",
    "                    for syn in synsets:\n",
    "                        score += syn.pos_score() - syn.neg_score()\n",
    "                    sent_score.append(score / len(synsets))\n",
    "        if (len(sent_score)==0 or len(sent_score)==1):\n",
    "            return (float(0.0))\n",
    "        else:\n",
    "            return (sum([word_score for word_score in sent_score]) / (len(sent_score)))\n",
    "\n",
    "def count_entities(col):\n",
    "    # set up tagger\n",
    "    os.environ['CLASSPATH'] = \"/Users/teresaborcuch/stanford-ner-2013-11-12/stanford-ner.jar\"\n",
    "    os.environ['STANFORD_MODELS'] = '/Users/teresaborcuch/stanford-ner-2013-11-12/classifiers'\n",
    "    st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "    tagged_titles = []\n",
    "    persons = []\n",
    "    places = []\n",
    "    male_counts = []\n",
    "    female_counts = []\n",
    "    male_names = names.words(\"male.txt\")\n",
    "    female_names = names.words(\"female.txt\")\n",
    "\n",
    "    for x in col:\n",
    "        tokens = word_tokenize(x)\n",
    "        tags = st.tag(tokens)\n",
    "        tagged_titles.append(tags)\n",
    "\n",
    "    for pair_list in tagged_titles:\n",
    "        person_count = 0\n",
    "        place_count = 0\n",
    "\n",
    "        for pair in pair_list:\n",
    "            m_count = 0\n",
    "            f_count = 0\n",
    "            if pair[1] == 'PERSON':\n",
    "                person_count +=1\n",
    "                if pair[0] in male_names:\n",
    "                    m_count +=1\n",
    "                elif pair[0] in female_names:\n",
    "                    f_count +=1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "            elif pair[1] == 'LOCATION':\n",
    "                place_count +=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        persons.append(person_count)\n",
    "        places.append(place_count)\n",
    "        male_counts.append(m_count)\n",
    "        female_counts.append(f_count)\n",
    "\n",
    "    return persons, places, male_counts, female_counts\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.encode('ascii','ignore')\n",
    "    lemmas = []\n",
    "\n",
    "    def lemmatize(token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        wnl = WordNetLemmatizer()\n",
    "        return wnl.lemmatize(token, tag)\n",
    "\n",
    "    for token, tag in pos_tag(wordpunct_tokenize(text)):\n",
    "        token = token.lower()\n",
    "        token = token.strip()\n",
    "        token = token.strip('_')\n",
    "        token = token.strip('*')\n",
    "\n",
    "        if token in sw.words('english'):\n",
    "            continue\n",
    "        if all(char in string.punctuation for char in token):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatize(token, tag)\n",
    "        lemmas.append(lemma)\n",
    "        lemma_string = ' '.join(lemmas)\n",
    "\n",
    "    return lemma_string\n",
    "\n",
    "def name_entities(data = None, section = None, source = None):\n",
    "    section_mask = (data['condensed_section'] == section)\n",
    "    source_mask = (data['source'] == source)\n",
    "\n",
    "    if section and source:\n",
    "        masked_data = data[section_mask & source_mask]\n",
    "\n",
    "    elif section:\n",
    "        masked_data = data[section_mask]\n",
    "\n",
    "    elif source:\n",
    "        masked_data = data[source_mask]\n",
    "\n",
    "    else:\n",
    "        masked_data = data\n",
    "\n",
    "    # set up tagger\n",
    "    os.environ['CLASSPATH'] = \"/Users/teresaborcuch/stanford-ner-2013-11-12/stanford-ner.jar\"\n",
    "    os.environ['STANFORD_MODELS'] = '/Users/teresaborcuch/stanford-ner-2013-11-12/classifiers'\n",
    "    st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "    # dictionaries to hold counts of entities\n",
    "    person_dict = {}\n",
    "    place_dict = {}\n",
    "\n",
    "    for x in masked_data['body']:\n",
    "        tokens = word_tokenize(x)\n",
    "        tags = st.tag(tokens)\n",
    "        for pair in tags:\n",
    "            if pair[1] == 'PERSON':\n",
    "                if pair[0] not in person_dict.keys():\n",
    "                    person_dict[pair[0]] = 1\n",
    "                else:\n",
    "                    person_dict[pair[0]] +=1\n",
    "            elif pair[1] == 'LOCATION':\n",
    "                if pair[0] not in place_dict.keys():\n",
    "                    place_dict[pair[0]] = 1\n",
    "                else:\n",
    "                    place_dict[pair[0]] += 1\n",
    "\n",
    "    return person_dict, place_dict\n",
    "\n",
    "def evaluate_topic(data = None, section = None, source = None, topic = None):\n",
    "    topic_scores = []\n",
    "    nontopic_scores = []\n",
    "\n",
    "    section_mask = (data['condensed_section'] == section)\n",
    "    source_mask = (data['source'] == source)\n",
    "\n",
    "    if section and source:\n",
    "        masked_data = data[section_mask & source_mask]\n",
    "\n",
    "    elif section:\n",
    "        masked_data = data[section_mask]\n",
    "\n",
    "    elif source:\n",
    "        masked_data = data[source_mask]\n",
    "\n",
    "    else:\n",
    "        masked_data = data\n",
    "\n",
    "    for i, row in masked_data.iterrows():\n",
    "\n",
    "        if topic in row[2]:\n",
    "            topic_scores.append(row[6])\n",
    "\n",
    "        else:\n",
    "            nontopic_scores.append(row[6])\n",
    "\n",
    "    score_dict = {'topic': topic_scores, 'nontopic': nontopic_scores}\n",
    "\n",
    "def transform_data(data, vocab = None):\n",
    "    transform_pipe = FeatureUnion([('titles', TitleTransformer()),\n",
    "                                    ('bodies', BodyTransformer()),\n",
    "                                    ('sources', Pipeline([('itemselector', ItemSelector(key = 'source')),\n",
    "                                                         ('dummy', DummyMaker())])),\n",
    "                                   ('tfidf-pipe', Pipeline([('itemselector', ItemSelector(key = 'body')),\n",
    "                                                              ('tfidf', TfidfDF(vocabulary = vocab))                     ]))\n",
    "                                  ])\n",
    "\n",
    "    Xt = transform_pipe.fit_transform(data)\n",
    "    return Xt\n",
    "\n",
    "\n",
    "def get_vocabulary(X, y):\n",
    "    scaler = MinMaxScaler()\n",
    "    Xt = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "    selector = SelectKbest(scoring_func = chi2, k = 5000)\n",
    "    selector.fit_transform(Xt, y)\n",
    "    vocabulary = X.columns[selector.get_support()]\n",
    "    for x in ['body_persons', 'body_places','body_men', 'body_women', 'body_ss',\n",
    "                'title_persons', 'title_places', 'title_men', 'title_women', 'title_ss',\n",
    "                'source_NYT', 'source_WP', 'source_Fox']:\n",
    "        if x in vocabulary:\n",
    "            vocabulary = vocabulary.remove(x)\n",
    "    return vocabulary\n",
    "\n",
    "#def build_model(X, y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ArticleData().call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_data(data, vocab = None):\n",
    "    transform_pipe = FeatureUnion([('titles', TitleTransformer()),\n",
    "                                    ('bodies', BodyTransformer()),\n",
    "                                    ('sources', Pipeline([('itemselector', ItemSelector(key = 'source')),\n",
    "                                                         ('dummy', DummyMaker())])),\n",
    "                                   ('tfidf-pipe', Pipeline([('itemselector', ItemSelector(key = 'body')),\n",
    "                                                              ('tfidf', TfidfVectorizer(preprocessor = tokenize, ngram_range = (1,2)))                     ]))\n",
    "                                  ])\n",
    "\n",
    "    Xt = transform_pipe.fit_transform(data)\n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = transform_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_persons</th>\n",
       "      <th>title_places</th>\n",
       "      <th>title_men</th>\n",
       "      <th>title_women</th>\n",
       "      <th>title_ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.023148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.034722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title_persons  title_places  title_men  title_women  title_ss\n",
       "0              0             0          0            0  0.052484\n",
       "1              0             0          0            0 -0.023148\n",
       "2              0             0          0            0  0.041667\n",
       "3              0             0          0            0 -0.034722\n",
       "4              0             0          0            0  0.084028"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = TitleTransformer()\n",
    "tt.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_persons</th>\n",
       "      <th>body_places</th>\n",
       "      <th>body_men</th>\n",
       "      <th>body_women</th>\n",
       "      <th>body_ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_persons  body_places  body_men  body_women   body_ss\n",
       "0            42            5         0           0 -0.000025\n",
       "1            17            8         0           0  0.016240\n",
       "2            13            2         0           0  0.020668\n",
       "3            24            9         0           0  0.000946\n",
       "4            80            4         0           0  0.027508"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = BodyTransformer()\n",
    "bt.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>condensed_section</th>\n",
       "      <th>date</th>\n",
       "      <th>section</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   body  condensed_section  date  section  source  title\n",
       "0   0.0                0.0   0.0      0.0     0.0    1.0\n",
       "1   0.0                0.0   1.0      0.0     0.0    0.0\n",
       "2   1.0                0.0   0.0      0.0     0.0    0.0\n",
       "3   0.0                0.0   0.0      1.0     0.0    0.0\n",
       "4   0.0                0.0   0.0      0.0     1.0    0.0\n",
       "5   0.0                1.0   0.0      0.0     0.0    0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfDF()\n",
    "tf.fit_transform(test_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
